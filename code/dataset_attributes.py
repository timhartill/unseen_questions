#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Sep 21 13:19:09 2021

@author: Tim Hartill

Common parameter definitions


Edit this file to add new datasets to evaluation/similarity routines:
    
- Add to dev_eval to produce preds, metrics for a dataset's dev.tsv - each dataset must be in either dev_eval or test_eval but not both!
- Add to test_eval to produce preds, metrics for a dataset's test.tsv - each dataset must be in either dev_eval or test_eval but not both!
- Add to dataset_attribs to include in eval and/or similarity calc plus to configure the set of metrics to be produced for a given dataset
- Add to replace_sim_with to use sembs from another dataset as proxy in calculating similarity for a given dataset
- Edit the following to add/remove datasets from sets of current output reports:
    unifiedqa_unseen_4      # unseen eval dataset
    unifiedqa_unseen_4_map  # must configure this to identify whether dev.tsv or test.tsv is the file to be used for calculation
    unifiedqa_unseen_6      # filtered versions of unseen eval datasets (not currently used)
    unifiedqa_seen_1        # datasets used in training but that we wish to evaluate for various reasons anyway
- Edit UQA_DIR to point to base directory for unified-qa formatted datasets.
- Edit create_datasets_dynamic to add new datasets to dynamically create explations for (i.e from q[+mc]->a make q[+mc]+e->a). 
    Datasets added here must be in dev_eval/test_eval and in dataset_attribs..
    Dynamically created versions i.e /UQA_DIR/qasc_svised_expl_ans_modeloutputdir_timestamp will be added to dev_eval/test_eval and dataset_attribs when this module is loaded..

After configuring as specified above, to run eval then:
- Edit/run one of the "runevalall....sh" scripts making sure to include flags: --do_predict_all --calc_metrics_all --add_only_missing
    - do_predict_all: Generates a predictions file with naming convention: dev|test_dataset name_predictions.json
                      Does so for each dataset specified below in dev_eval (for dev.tsv) and test_eval (for test.tsv)
                      Also outputs a file 'eval_results.csv' containing EM for each dataset as an easily viewable smoke test
    - calc_metrics_all: Creates and/or adds metrics for each dataset into "eval_metrics.json" from the prediction files generated by --do_predict_all
    - add_only_missing: Where specified will only incrementally add new datasets for which predictions/metrics do not alreay appear in the output dir
- Load eval_metrics.py into an interactive editor such as spyder and follow the instructions in respective docstrings to 
  configure/run eg run_all(logdir, results_list, include_list=['unseen4', 'seen1']):

                                      Model 1   Model 2     ...
      ds name 1  preferred metric     0.5       0.7         ...
      ds name 2  preferred metric     0.43      0.65        ...

    and/or  eg output_summary(logdir, results_list, include_list, number_samples=3, metric='ALL', outname='eval_dataset_performance_summary.txt')

        unifiedqa_bart_large_s6_v5_musique_qa_decomp_ans_plus_all_decomps: EVAL DATASET:musique_mu_dev_qa
        OVERALL METRICS: EM_score: 0.00   F1_score: 2.64
        
        Random Samples:
        [410] INPUT: Who did the creator of Derech Mitzvosecha follow? \n
        LABL: Dovber Schneuri
        PRED: kenneth bi ## who is derech mitzvosecha by? hans holbein ## hansholbein >> followed by? karl renner
        SAMPLE METRICS: EM_score: 0.00   F1_score: 0.00

- Run metrics in similarity buckets ...

"""

import os
import fnmatch

SVISED_EXPL_ANS = '_dyn_expl_ans_'
selfsupervisedkey = '_selfsvised'   # dataset names ending in this will be processed as self supervised
add_explanationkey = 'Add Explanation:'
EXPL_COMP_KEY = '_expl_components'
special_tokens_dict = {'additional_special_tokens':['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']}  # for individual digit tokenisation

if os.environ.get('UQA_DIR') is not None:
    UQA_DIR = os.environ.get('UQA_DIR') + '/'  #'/data/thar011/data/unifiedqa/' # datasets base directory
else:
    assert False, "ERROR: Set the environment variable 'UQA_DIR' to the base directory of tsv-formatted datasets: export UQA_DIR=/parentdir/datasets "


#Add to this list to create predictions/calc metrics for corresponding dev.tsv:
dev_eval = ['newsqa', 'quoref', 'contrast_sets_quoref', 'ropes', 'contrast_sets_ropes', 
            'boolq_np', 'contrast_sets_boolq', 'physical_iqa', 
            'social_iqa', 'squad1_1_titlereformat', 'squad2_titlereformat', 'boolq',  
            'winogrande_xl', 'tatqa',
            'csqa2', 'csqa2_fullwiki_bs150_noimplrel', 'csqa2_fullwiki_bs150_noimplrel_maxp4', 
            'csqa2_impl_rels', 'csqa2_fullwiki_bs150_implrel', 'csqa2_fullwiki_bs150_implrel_origq',
            'creak_hard', 'creak_initial_context', 'creak_fullwiki_bs150_frominitctx', 
            'creak_od_ans', 'creak_fullwiki_bs150_noimplrel', 'creak_fullwiki_bs150_noimplrel_maxp4', 
            'creak_od_ans_impl_rels', 'creak_fullwiki_bs150_implrel', 'creak_fullwiki_bs150_implrel_origq', 
            'creak_fullwiki_bs150_implrel_bestsentsonly_origq', 'creak_fullwiki_bs150_implrel_bestsents_maxp4_origq', 'creak_fullwiki_bs150_implrel_maxp4_origq',
            'creak_contrast_set_od_ans', 'creak_contrast_set_hard', 'creak_contrast_set_initial_context',
            'drop', 'contrast_sets_drop',
            'commonsenseqa',
            'commonsenseqa_fullwiki_bs150_noimplrel', 'commonsenseqa_fullwiki_bs150_implrel', 'commonsenseqa_fullwiki_bs150_implrel_origq',
            'musique_mu_dev_odv2', 'musique_mu_dev_parasv2',
            'musique_mu_dev_odv2_fullwiki_bs150', 'musique_mu_dev_inital_contextv2', 'musique_mu_dev_inital_contextv2_fullwiki_bs150',
            'strategy_qa_bigbench_od_ans', 'strategy_qa_bigbench_expl_ans',
            'strategy_qa_bigbench_gold_context_0', 'strategy_qa_bigbench_gold_context_1', 'strategy_qa_bigbench_gold_context_2',
            'strategy_qa_bigbench_fullwiki_bs150_noimplrel', 'strategy_qa_bigbench_fullwiki_bs150_implrel', 'strategy_qa_bigbench_fullwiki_bs150_implrel_origq', 
            'fever_hard',
            'hover_hard', 'hover_fullwiki_bs60', 'hover_fullwiki_bs60_maxp4',
            'qasc', 'qasc_with_ir', 'qasc_fullwiki_bs60', 'qasc_fullwiki_bs60_maxp4',
            'hpqa_hard', 'hpqa_fullwiki_bs60', 'hpqa_fullwiki_bs60_maxp4', 
            'musique_hard', 'musique_qa_fullwiki_bs60', 'musique_qa_fullwiki_bs60_maxp4', 
            'nq_hard', 'nq_open_od_ans', 'nq_open_fullwiki_bs60', 'nq_open_fullwiki_bs60_maxp4',
            ]


#Add to this list to create predictions/calc metrics for corresponding test.tsv:
test_eval = ['openbookqa', 'openbookqa_with_ir', 'arc_easy', 'arc_easy_with_ir', 'arc_hard', 
             'arc_hard_with_ir', 'ai2_science_elementary', 'ai2_science_middle', 'race_string',  
             'mmlu_elementary_to_college_math_test', 
             'arc_da_expl_ans', 'arc_da_od_ans', 
             'arc_da_od_ans_fullwiki_bs150',
             'iirc_od_ans', 'iirc_gold_context', 'iirc_initial_context',
             'iirc_od_ans_fullwiki_bs150', 'iirc_initial_context_fullwiki_bs150',
             ]



#Map dataset types to relevant metrics to calculate (and specify referred reporting metric)
metric_groups = {
    'EX': {'compute':['EM', 'F1', 'RL'], 'prefer':'F1'},
    'AB': {'compute':['EM', 'F1', 'RL'], 'prefer':'RL'},
    'MC': {'compute':['EM', 'F1', 'SS'], 'prefer':'SS'},
    'YN': {'compute':['EM', 'F1', 'YN'], 'prefer':'YN'},
    'DC': {'compute':['EM', 'F1A', 'F1DA', 'SARID', 'SARIDA'], 'prefer':'F1A'}  #F1A = F1 on answer only so comparable to other datasets
}

########################################################
#Map datasets to dataset types and optional override for preferred reporting metric (must be one in above 'compute' key)
#NOTE: Any dataset used in evaluation metrics calculation and/or similarity calculation must be added to dataset_attribs
########################################################

dataset_attribs = {
    'ai2_science_elementary': {'type':'MC', 'prefer':''},
    'ai2_science_middle': {'type':'MC', 'prefer':''},
    'ambigqa': {'type':'AB', 'prefer':''},
    'ambigqa_dedup': {'type':'AB', 'prefer':''},
    'arc_easy': {'type':'MC', 'prefer':''},
    'arc_easy_dev': {'type':'MC', 'prefer':''},
    'arc_easy_with_ir': {'type':'MC', 'prefer':''},
    'arc_easy_with_ir_dev': {'type':'MC', 'prefer':''},
    'arc_hard': {'type':'MC', 'prefer':''},
    'arc_hard_dev': {'type':'MC', 'prefer':''},
    'arc_hard_with_ir': {'type':'MC', 'prefer':''},
    'arc_hard_with_ir_dev': {'type':'MC', 'prefer':''},
    'worldtree_mc_ans': {'type':'MC', 'prefer':''},        
    'boolq': {'type':'YN', 'prefer':''},
    'boolq_np': {'type':'YN', 'prefer':''},
    'boolq_np_dedup': {'type':'YN', 'prefer':''},
    'commonsenseqa': {'type':'MC', 'prefer':''},
    'commonsenseqa_test': {'type':'MC', 'prefer':''},
    'commonsenseqa_fullwiki_bs150_noimplrel': {'type':'MC', 'prefer':''}, 
    'commonsenseqa_fullwiki_bs150_implrel': {'type':'MC', 'prefer':''}, 
    'commonsenseqa_fullwiki_bs150_implrel_origq': {'type':'MC', 'prefer':''},
    'contrast_sets_boolq': {'type':'YN', 'prefer':''},
    'contrast_sets_boolq_dedup': {'type':'YN', 'prefer':''},
    'contrast_sets_drop': {'type':'AB', 'prefer':'F1'},
    'contrast_sets_drop_dedup': {'type':'AB', 'prefer':'F1'},
    'contrast_sets_quoref': {'type':'EX', 'prefer':''},
    'contrast_sets_quoref_dedup': {'type':'EX', 'prefer':''},
    'contrast_sets_ropes': {'type':'EX', 'prefer':''},
    'contrast_sets_ropes_dedup': {'type':'EX', 'prefer':''},
    'drop': {'type':'AB', 'prefer':'F1'},
    'drop_dedup': {'type':'AB', 'prefer':'F1'},
    'mctest': {'type':'MC', 'prefer':''},
    'mctest_corrected_the_separator': {'type':'MC', 'prefer':''},
    'multirc': {'type':'YN', 'prefer':''},
    'narrativeqa': {'type':'AB', 'prefer':''},
    'narrativeqa_dev': {'type':'AB', 'prefer':''},
    'natural_questions': {'type':'AB', 'prefer':'EM'},
    'natural_questions_direct_ans': {'type':'AB', 'prefer':'EM'},
    'natural_questions_direct_ans_test': {'type':'AB', 'prefer':'EM'},
    'natural_questions_with_dpr_para': {'type':'AB', 'prefer':'EM'},
    'natural_questions_with_dpr_para_test': {'type':'AB', 'prefer':'EM'},
    'newsqa': {'type':'EX', 'prefer':''},
    'openbookqa': {'type':'MC', 'prefer':''},
    'openbookqa_dev': {'type':'MC', 'prefer':''},
    'openbookqa_with_ir': {'type':'MC', 'prefer':''},
    'openbookqa_with_ir_dev': {'type':'MC', 'prefer':''},
    'physical_iqa': {'type':'MC', 'prefer':''},
    'qasc_mc_ans': {'type':'MC', 'prefer':''},
    'qasc': {'type':'MC', 'prefer':''},
    'qasc_test': {'type':'MC', 'prefer':''},
    'qasc_with_ir': {'type':'MC', 'prefer':''},
    'qasc_with_ir_test': {'type':'MC', 'prefer':''},
    'qasc_fullwiki_bs60': {'type':'MC', 'prefer':''}, 
    'qasc_fullwiki_bs60_maxp4': {'type':'MC', 'prefer':''},
    'quoref': {'type':'EX', 'prefer':''},
    'quoref_dedup': {'type':'EX', 'prefer':''},
    'race_string': {'type':'MC', 'prefer':''},
    'race_string_dev': {'type':'MC', 'prefer':''},
    'ropes': {'type':'EX', 'prefer':''},
    'social_iqa': {'type':'MC', 'prefer':''},
    'social_iqa_dedup': {'type':'MC', 'prefer':''},
    'squad1_1_titlereformat': {'type':'EX', 'prefer':''},
    'squad2_titlereformat': {'type':'EX', 'prefer':''},
    'winogrande_l': {'type':'MC', 'prefer':''},
    'winogrande_m': {'type':'MC', 'prefer':''},
    'winogrande_s': {'type':'MC', 'prefer':''},
    'winogrande_test': {'type':'MC', 'prefer':''},
    'winogrande_xl': {'type':'MC', 'prefer':''},
    'winogrande_xs': {'type':'MC', 'prefer':''},
    'mmlu_elementary_mathematics_test': {'type':'MC', 'prefer':''},
    'mmlu_elementary_mathematics_test_dedup': {'type':'MC', 'prefer':''},
     'mmlu_business_ethics_test': {'type':'MC', 'prefer':''},
     'mmlu_professional_accounting_test': {'type':'MC', 'prefer':''},
     'mmlu_college_mathematics_test': {'type':'MC', 'prefer':''},
     'mmlu_public_relations_test': {'type':'MC', 'prefer':''},
     'mmlu_public_relations_test_dedup': {'type':'MC', 'prefer':''}, 
     'mmlu_philosophy_test': {'type':'MC', 'prefer':''},
     'mmlu_high_school_government_and_politics_test': {'type':'MC', 'prefer':''},
     'mmlu_professional_medicine_test': {'type':'MC', 'prefer':''},
     'mmlu_high_school_biology_test': {'type':'MC', 'prefer':''},
     'mmlu_moral_disputes_test': {'type':'MC', 'prefer':''},
     'mmlu_moral_scenarios_test': {'type':'MC', 'prefer':''},
     'mmlu_clinical_knowledge_test': {'type':'MC', 'prefer':''},
     'mmlu_college_computer_science_test': {'type':'MC', 'prefer':''},
     'mmlu_jurisprudence_test': {'type':'MC', 'prefer':''},
     'mmlu_logical_fallacies_test': {'type':'MC', 'prefer':''},   
     'mmlu_us_foreign_policy_test': {'type':'MC', 'prefer':''},
     'mmlu_us_foreign_policy_test_dedup': {'type':'MC', 'prefer':''},
     'mmlu_high_school_statistics_test': {'type':'MC', 'prefer':''},
     'mmlu_virology_test': {'type':'MC', 'prefer':''},
     'mmlu_formal_logic_test': {'type':'MC', 'prefer':''},
     'mmlu_security_studies_test': {'type':'MC', 'prefer':''},
     'mmlu_machine_learning_test': {'type':'MC', 'prefer':''},
     'mmlu_high_school_us_history_test': {'type':'MC', 'prefer':''},
     'mmlu_world_religions_test': {'type':'MC', 'prefer':''},
     'mmlu_high_school_chemistry_test': {'type':'MC', 'prefer':''},
     'mmlu_prehistory_test': {'type':'MC', 'prefer':''},
     'mmlu_electrical_engineering_test': {'type':'MC', 'prefer':''},
     'mmlu_high_school_european_history_test': {'type':'MC', 'prefer':''},
     'mmlu_high_school_psychology_test': {'type':'MC', 'prefer':''},
     'mmlu_high_school_psychology_test_dedup': {'type':'MC', 'prefer':''},
     'mmlu_high_school_world_history_test': {'type':'MC', 'prefer':''},
     'mmlu_high_school_geography_test': {'type':'MC', 'prefer':''},
     'mmlu_high_school_computer_science_test': {'type':'MC', 'prefer':''},
     'mmlu_human_aging_test': {'type':'MC', 'prefer':''},
     'mmlu_marketing_test': {'type':'MC', 'prefer':''},
     'mmlu_high_school_mathematics_test': {'type':'MC', 'prefer':''},
     'mmlu_conceptual_physics_test': {'type':'MC', 'prefer':''},
     'mmlu_abstract_algebra_test': {'type':'MC', 'prefer':''},
     'mmlu_professional_psychology_test': {'type':'MC', 'prefer':''},
     'mmlu_professional_psychology_test_dedup': {'type':'MC', 'prefer':''},
     'mmlu_management_test': {'type':'MC', 'prefer':''},
     'mmlu_high_school_macroeconomics_test': {'type':'MC', 'prefer':''},
     'mmlu_sociology_test': {'type':'MC', 'prefer':''},
     'mmlu_nutrition_test': {'type':'MC', 'prefer':''},
     'mmlu_college_biology_test': {'type':'MC', 'prefer':''},
     'mmlu_professional_law_test': {'type':'MC', 'prefer':''},
     'mmlu_astronomy_test': {'type':'MC', 'prefer':''},
     'mmlu_college_physics_test': {'type':'MC', 'prefer':''},
     'mmlu_college_physics_test_dedup': {'type':'MC', 'prefer':''},
     'mmlu_miscellaneous_test': {'type':'MC', 'prefer':''},
     'mmlu_high_school_microeconomics_test': {'type':'MC', 'prefer':''},
     'mmlu_computer_security_test': {'type':'MC', 'prefer':''},
     'mmlu_international_law_test': {'type':'MC', 'prefer':''},
     'mmlu_global_facts_test': {'type':'MC', 'prefer':''},
     'mmlu_human_sexuality_test': {'type':'MC', 'prefer':''},
     'mmlu_econometrics_test': {'type':'MC', 'prefer':''},
     'mmlu_anatomy_test': {'type':'MC', 'prefer':''},
     'mmlu_medical_genetics_test': {'type':'MC', 'prefer':''},
     'mmlu_college_medicine_test': {'type':'MC', 'prefer':''},
     'mmlu_high_school_physics_test': {'type':'MC', 'prefer':''},
     'mmlu_college_chemistry_test': {'type':'MC', 'prefer':''},
     'mmlu_elementary_to_college_math_test': {'type':'MC', 'prefer':''},
     'synthetic_numeric': {'type':'AB', 'prefer':'EM'},
     'synthetic_textual': {'type':'AB', 'prefer':'EM'},
    'drop_dedup_lowsim_uqa': {'type':'AB', 'prefer':'F1'},
    'contrast_sets_drop_dedup_lowsim_uqa': {'type':'AB', 'prefer':'F1'},
    'mmlu_elementary_to_college_math_test_lowsim_uqa': {'type':'MC', 'prefer':''},
    'physical_iqa_lowsim_uqa': {'type':'MC', 'prefer':''},
    'social_iqa_dedup_lowsim_uqa': {'type':'MC', 'prefer':''},
    'commonsenseqa_lowsim_uqa': {'type':'MC', 'prefer':''},
    'qasc_lowsim_uqa': {'type':'MC', 'prefer':''},
    'qasc_with_ir_lowsim_uqa': {'type':'MC', 'prefer':''},
    'ropes_lowsim_uqa': {'type':'EX', 'prefer':''},
    'newsqa_lowsim_uqa': {'type':'EX', 'prefer':''},
    'drop_dedup_lowsim_tdnd': {'type':'AB', 'prefer':'F1'},
    'contrast_sets_drop_dedup_lowsim_tdnd': {'type':'AB', 'prefer':'F1'},
    'mmlu_elementary_to_college_math_test_lowsim_tdnd': {'type':'MC', 'prefer':''},
    'physical_iqa_lowsim_tdnd': {'type':'MC', 'prefer':''},
    'social_iqa_dedup_lowsim_tdnd': {'type':'MC', 'prefer':''},
    'commonsenseqa_lowsim_tdnd': {'type':'MC', 'prefer':''},
    'qasc_lowsim_tdnd': {'type':'MC', 'prefer':''},
    'qasc_with_ir_lowsim_tdnd': {'type':'MC', 'prefer':''},
    'ropes_lowsim_tdnd': {'type':'EX', 'prefer':''},
    'newsqa_lowsim_tdnd': {'type':'EX', 'prefer':''},
    'strategy_qa': {'type':'MC', 'prefer':''},
    'cwwv': {'type':'MC', 'prefer':''},
    'cskg': {'type':'MC', 'prefer':''},
    'atomic': {'type':'MC', 'prefer':''},
    'musique_qa': {'type':'EX', 'prefer':''},
    'musique_qa_paras': {'type':'EX', 'prefer':''},
    'musique_mu_dev_qa': {'type':'EX', 'prefer':''},
    'musique_mu_dev_qa_paras': {'type':'EX', 'prefer':''},
    'musique_qa_decomp_ans': {'type':'DC', 'prefer':''},
    'musique_qa_paras_decomp_ans': {'type':'DC', 'prefer':''},
    'musique_mu_dev_qa_decomp_ans': {'type':'DC', 'prefer':''},
    'musique_mu_dev_qa_paras_decomp_ans': {'type':'DC', 'prefer':''},
    'musique_qa_plus_qa_decomp_ans': {'type':'EX', 'prefer':''},
    'musique_qa_plus_qa_decomp_ans_full': {'type':'EX', 'prefer':''},
    'musique_qa_paras_plus_qa_paras_decomp_ans': {'type':'EX', 'prefer':''},
    'musique_qa_paras_plus_qa_paras_decomp_ans_full': {'type':'EX', 'prefer':''},
    'musique_mu_dev_qa_decomp_context': {'type':'EX', 'prefer':''},
    'musique_mu_dev_qa_expl_ans': {'type':'EX', 'prefer':''},
    'musique_hard': {'type':'EX', 'prefer':''}, 
    'musique_qa_fullwiki_bs60': {'type':'EX', 'prefer':''}, 
    'musique_qa_fullwiki_bs60_maxp4': {'type':'EX', 'prefer':''},
    'musique_mu_dev_odv2': {'type':'EX', 'prefer':''}, 
    'musique_mu_dev_parasv2': {'type':'EX', 'prefer':''},
    'musique_mu_dev_odv2_fullwiki_bs150': {'type':'EX', 'prefer':''},
    'musique_mu_dev_inital_contextv2': {'type':'EX', 'prefer':''},
    'musique_mu_dev_inital_contextv2_fullwiki_bs150': {'type':'EX', 'prefer':''},
    'strategy_qa_od_ans': {'type':'YN', 'prefer':''},
    'strategy_qa_expl_ans': {'type':'YN', 'prefer':''},
    'strategy_qa_bigbench_od_ans': {'type':'YN', 'prefer':''},
    'strategy_qa_bigbench_expl_ans': {'type':'YN', 'prefer':''},
    'strategy_qa_bigbench_fullwiki_bs150_noimplrel': {'type':'YN', 'prefer':''}, 
    'strategy_qa_bigbench_fullwiki_bs150_implrel': {'type':'YN', 'prefer':''}, 
    'strategy_qa_bigbench_fullwiki_bs150_implrel_origq': {'type':'YN', 'prefer':''}, 
    'strategy_qa_bigbench_gold_context_0': {'type':'YN', 'prefer':''}, 
    'strategy_qa_bigbench_gold_context_1': {'type':'YN', 'prefer':''}, 
    'strategy_qa_bigbench_gold_context_2': {'type':'YN', 'prefer':''},
    'nq_hard': {'type':'EX', 'prefer':'EM'},
    'nq_open_od_ans': {'type':'EX', 'prefer':'EM'}, 
    'nq_open_fullwiki_bs60': {'type':'EX', 'prefer':'EM'}, 
    'nq_open_fullwiki_bs60_maxp4': {'type':'EX', 'prefer':'EM'},
    'tatqa': {'type':'EX', 'prefer':''},
    'arc_da_expl_ans': {'type':'EX', 'prefer':''}, 
    'arc_da_od_ans': {'type':'EX', 'prefer':''},
    'arc_da_unfiltered_od_ans': {'type':'EX', 'prefer':''},
    'arc_da_od_ans_fullwiki_bs150': {'type':'EX', 'prefer':''},
    'iirc_od_ans': {'type':'EX', 'prefer':''},
    'iirc_gold_context': {'type':'EX', 'prefer':''},
    'iirc_initial_context': {'type':'EX', 'prefer':''},
    'iirc_od_ans_fullwiki_bs150': {'type':'EX', 'prefer':''}, 
    'iirc_initial_context_fullwiki_bs150': {'type':'EX', 'prefer':''},
    'csqa2': {'type':'YN', 'prefer':''},
    'csqa2_fullwiki_bs150_noimplrel': {'type':'YN', 'prefer':''},
    'csqa2_fullwiki_bs150_noimplrel_maxp4': {'type':'YN', 'prefer':''},
    'csqa2_impl_rels': {'type':'YN', 'prefer':''},
    'csqa2_fullwiki_bs150_implrel': {'type':'YN', 'prefer':''},
    'csqa2_fullwiki_bs150_implrel_origq': {'type': 'YN', 'prefer':''},
    'creak_hard': {'type':'YN', 'prefer':''},
    'creak_initial_context': {'type':'YN', 'prefer':''},
    'creak_fullwiki_bs150_frominitctx': {'type':'YN', 'prefer':''},
    'creak_od_ans': {'type':'YN', 'prefer':''},
    'creak_fullwiki_bs150_noimplrel': {'type':'YN', 'prefer':''},
    'creak_fullwiki_bs150_noimplrel_maxp4': {'type':'YN', 'prefer':''},
    'creak_od_ans_impl_rels': {'type':'YN', 'prefer':''},
    'creak_fullwiki_bs150_implrel': {'type':'YN', 'prefer':''},
    'creak_fullwiki_bs150_implrel_origq': {'type':'YN', 'prefer':''},
    'creak_fullwiki_bs150_implrel_bestsentsonly_origq': {'type':'YN', 'prefer':''},
    'creak_fullwiki_bs150_implrel_bestsents_maxp4_origq': {'type':'YN', 'prefer':''},
    'creak_fullwiki_bs150_implrel_maxp4_origq': {'type':'YN', 'prefer':''},
    'creak_contrast_set_od_ans': {'type':'YN', 'prefer':''},
    'creak_contrast_set_hard': {'type':'YN', 'prefer':''},
    'creak_contrast_set_initial_context': {'type':'YN', 'prefer':''},
    'fever_hard': {'type':'YN', 'prefer':''},
    'hover_hard': {'type':'YN', 'prefer':''}, 
    'hover_fullwiki_bs60': {'type':'YN', 'prefer':''}, 
    'hover_fullwiki_bs60_maxp4': {'type':'YN', 'prefer':''},
    'hpqa_hard': {'type':'EX', 'prefer':''}, 
    'hpqa_fullwiki_bs60': {'type':'EX', 'prefer':''}, 
    'hpqa_fullwiki_bs60_maxp4': {'type':'EX', 'prefer':''},
    }



#################################################
# Groups of datasets for use in 'mixture' parameter to save typing each in individually 
#################################################

unifiedqa_base_train = ["narrativeqa", "ai2_science_middle", "ai2_science_elementary",
                        "arc_hard", "arc_easy", "mctest_corrected_the_separator",
                        "squad1_1_titlereformat", "squad2_titlereformat", "boolq", "race_string", "openbookqa"]

# PReasM / turning tables datasets:
tt_base_train = [   "tt_arithmetic_addition",
                    "tt_arithmetic_superlatives",
                    "tt_composition",
                    "tt_composition_2_hop",
                    "tt_conjunction",
                    "tt_counting",
                    "tt_every_quantifier",
                    "tt_most_quantifier",
                    "tt_numeric_comparison_boolean",
                    "tt_numeric_superlatives",
                    "tt_only_quantifier",
                    "tt_temporal_comparison",
                    "tt_temporal_comparison_boolean",
                    "tt_temporal_difference",
                    "tt_temporal_superlatives", ]

# Poet-SQL: separated datasets:
poet_base_train = [ "poetsql_multi",
                    "poetsql_select_abs",
                    "poetsql_select_arith",
                    "poetsql_select_count",
                    "poetsql_select_max",
                    "poetsql_select_min",
                    "poetsql_select_sum",
                    "poetsql_single",  ]

# New synthetic numeric datasets
synth_num_base_train = ["synthetic_num_arg_min_max",
                        "synthetic_num_date_diff",
                        "synthetic_num_date_min_max",
                        "synthetic_num_min_max_avg",
                        "synthetic_num_percent",
                        "synthetic_num_signed_arith",
                        "synthetic_num_yn_dates",
                        "synthetic_num_yn_nums", ]

# q+paras->a
q_paras_train = ["narrativeqa", "squad1_1_titlereformat", "squad2_titlereformat", "boolq", "boolq_np",
                 "ropes", "newsqa", "quoref", "multirc", 
                 "adversarialqa_all", "qaconv", "pubmedqa_pqal_short_ans", "record_extractive", "tweetqa", "tatqa",
                 "creak_hard", "hpqa_hard", "musique_hard", "tqa_hard", "nq_hard", "hover_hard", "fever_hard"]

# q+paras -> <No Answer>
q_paras_noanswer_train = ["noanswer_hpqa_fever_hover_hard", "no_answer_musique_hard"]

# q->a
q_od_train = ["tqa_open_od_ans", "nq_open_od_ans", "csqa2", "creak_od_ans", "twentyquestions"]

# q+mc->a
q_mc_train = ["ai2_science_middle", "ai2_science_elementary", "arc_hard", "arc_easy", "openbookqa", "qasc", 
              "physical_iqa", "winogrande_xl", ]

# q+mc+paras->a
q_mc_paras_train = ["mctest_corrected_the_separator", "race_string", "openbookqa_with_ir",
                    "arc_easy_with_ir", "arc_hard_with_ir", "qasc_with_ir", "social_iqa", 
                    "quail", "reclor", ]

#q[+mc]+our retrieved paras->a
q_ret_paras_train = ['csqa2_fullwiki_bs150_noimplrel', 'creak_fullwiki_bs150_noimplrel', 'hover_fullwiki_bs60', 
                     'qasc_fullwiki_bs60', 'hpqa_fullwiki_bs60', 'musique_qa_fullwiki_bs60', 'nq_open_fullwiki_bs60', ]

#q[+mc]+our retrieved paras with max 4 paras->a
q_ret_paras_maxp4_train = ['csqa2_fullwiki_bs150_noimplrel_maxp4', 'creak_fullwiki_bs150_noimplrel_maxp4', 'hover_fullwiki_bs60_maxp4', 
                           'qasc_fullwiki_bs60_maxp4', 'hpqa_fullwiki_bs60_maxp4', 'musique_qa_fullwiki_bs60_maxp4', 'nq_open_fullwiki_bs60_maxp4', ]


########################################################
# where same train dataset in difft formats, just calc sim against one format and map similarity for others against that...
########################################################

replace_sim_with = {'cwwv_selfsvised': 'cwwv', 
                'atomic_selfsvised': 'atomic', 
                'cwwv_premask_selfsvised': 'cwwv', 
                'atomic_premask_selfsvised': 'atomic',
                'musique_qa_decomp_ans': 'musique_qa',
                'musique_qa_plus_qa_decomp_ans': 'musique_qa',
                'musique_qa_paras_decomp_ans': 'musique_qa_paras',
                'musique_qa_paras_plus_qa_paras_decomp_ans': 'musique_qa_paras',
                'musique_qa_plus_qa_decomp_ans_full': 'musique_qa_full',
                'musique_qa_paras_plus_qa_paras_decomp_ans_full': 'musique_qa_paras_full',
                'musique_mu_dev_qa_decomp_ans': 'musique_mu_dev_qa',
                'musique_mu_dev_qa_paras_decomp_ans': 'musique_mu_dev_qa_paras',
                'strategy_qa_od_ans': 'strategy_qa',
                'strategy_qa_expl_ans': 'strategy_qa',
                'qasc_mc_ans': 'qasc',
                'musique_mu_dev_qa_expl_ans': 'musique_mu_dev_qa',
                }


########################################################
# Sets of Eval datasets used in generating reports...
########################################################

# Not used
unifiedqa_unseen_1 = []

# Not used
unifiedqa_unseen_2 = []

# Not used
unifiedqa_unseen_3 = []

# The unseen evaluation datasets
unifiedqa_unseen_4 = [
    'drop', 'contrast_sets_drop',
    'commonsenseqa',
    'commonsenseqa_fullwiki_bs150_noimplrel', 'commonsenseqa_fullwiki_bs150_implrel', 'commonsenseqa_fullwiki_bs150_implrel_origq',
    'musique_mu_dev_odv2', 'musique_mu_dev_parasv2', 'musique_mu_dev_inital_contextv2',
    'musique_mu_dev_odv2_fullwiki_bs150', 'musique_mu_dev_inital_contextv2_fullwiki_bs150',
    'arc_da_od_ans', 'arc_da_expl_ans', 
    'arc_da_od_ans_fullwiki_bs150',
    'strategy_qa_bigbench_od_ans', 'strategy_qa_bigbench_expl_ans',
    'strategy_qa_bigbench_gold_context_0', 'strategy_qa_bigbench_gold_context_1', 'strategy_qa_bigbench_gold_context_2',
    'strategy_qa_bigbench_fullwiki_bs150_noimplrel', 'strategy_qa_bigbench_fullwiki_bs150_implrel', 'strategy_qa_bigbench_fullwiki_bs150_implrel_origq', 
    'iirc_od_ans', 'iirc_gold_context', 'iirc_initial_context',
    'iirc_od_ans_fullwiki_bs150', 'iirc_initial_context_fullwiki_bs150',
    'mmlu_elementary_to_college_math_test',
    ]


# Note: This is only used in create_least_similar_versions.py and check_least_similar_answer.py
# These two py files have now been modified s.t. if a datasets isn't in this map, the file defaults to 'dev.tsv'..
# So only need to add datasets to this map if 'test.tsv' is the one needed..
unifiedqa_unseen_4_map = {
    'mmlu_elementary_to_college_math_test': 'test.tsv',
    'worldtree_mc_ans': 'test.tsv',
    'nq_open_od_ans': 'test.tsv',
    'arc_da_od_ans': 'test.tsv',
    'arc_da_unfiltered_od_ans': 'test.tsv',
    'arc_da_expl_ans': 'test.tsv',
    'iirc_od_ans': 'test.tsv', 
    'iirc_gold_context': 'test.tsv', 
    'iirc_initial_context': 'test.tsv',
    'iirc_od_ans_fullwiki_bs150': 'test.tsv', 
    'iirc_initial_context_fullwiki_bs150': 'test.tsv',
    'arc_da_od_ans_fullwiki_bs150': 'test.tsv',
    }


# The filtered versions of the 10 unseen datasets used in our paper
unifiedqa_unseen_6 = [
    'drop_dedup_lowsim_tdnd',
    'contrast_sets_drop_dedup_lowsim_tdnd',
    'mmlu_elementary_to_college_math_test_lowsim_tdnd',
    'physical_iqa_lowsim_tdnd',
    'social_iqa_dedup_lowsim_tdnd',
    'commonsenseqa_lowsim_tdnd',
    'qasc_lowsim_tdnd',
    'qasc_with_ir_lowsim_tdnd',
    'ropes_lowsim_tdnd',
    'newsqa_lowsim_tdnd'
    ]


# datasets unifiedqa trained on that we wish to evaluate for various reasons
unifiedqa_seen_1 = [
    'openbookqa',
    'openbookqa_with_ir',
    'arc_easy',
    'arc_easy_with_ir',
    'arc_hard',
    'arc_hard_with_ir',
    'ai2_science_elementary',
    'ai2_science_middle',
    'qasc', 'qasc_with_ir', 'qasc_fullwiki_bs60', 'qasc_fullwiki_bs60_maxp4',
    'race_string',
    'tatqa', 'newsqa', 'quoref', 'contrast_sets_quoref', 'ropes', 'contrast_sets_ropes', 
    'boolq_np', 'contrast_sets_boolq', 'physical_iqa', 
    'social_iqa', 'squad1_1_titlereformat', 'squad2_titlereformat', 'boolq', 
    'winogrande_xl',
    'csqa2', 'csqa2_fullwiki_bs150_noimplrel', 'csqa2_fullwiki_bs150_noimplrel_maxp4', 'csqa2_impl_rels', 'csqa2_fullwiki_bs150_implrel', 'csqa2_fullwiki_bs150_implrel_origq',
    'creak_hard', 'creak_initial_context', 'creak_fullwiki_bs150_frominitctx', 
    'creak_od_ans', 'creak_fullwiki_bs150_noimplrel', 'creak_fullwiki_bs150_noimplrel_maxp4', 
    'creak_od_ans_impl_rels', 'creak_fullwiki_bs150_implrel', 'creak_fullwiki_bs150_implrel_origq', 'creak_fullwiki_bs150_implrel_bestsentsonly_origq', 'creak_fullwiki_bs150_implrel_bestsents_maxp4_origq', 'creak_fullwiki_bs150_implrel_maxp4_origq',
    'creak_contrast_set_od_ans', 'creak_contrast_set_hard', 'creak_contrast_set_initial_context',
    'fever_hard',
    'hover_hard', 'hover_fullwiki_bs60', 'hover_fullwiki_bs60_maxp4',
    'hpqa_hard', 'hpqa_fullwiki_bs60', 'hpqa_fullwiki_bs60_maxp4', 
    'musique_hard', 'musique_qa_fullwiki_bs60', 'musique_qa_fullwiki_bs60_maxp4', 
    'nq_hard', 'nq_open_od_ans', 'nq_open_fullwiki_bs60', 'nq_open_fullwiki_bs60_maxp4',
    ]



# The 57 mmlu datasets. 
mmlu_unseen_1 = [
     'mmlu_elementary_mathematics_test',
     'mmlu_business_ethics_test',
     'mmlu_professional_accounting_test',
     'mmlu_college_mathematics_test',
     'mmlu_public_relations_test',
     'mmlu_philosophy_test',
     'mmlu_high_school_government_and_politics_test',
     'mmlu_professional_medicine_test',
     'mmlu_high_school_biology_test',
     'mmlu_moral_disputes_test',
     'mmlu_moral_scenarios_test',
     'mmlu_clinical_knowledge_test',
     'mmlu_college_computer_science_test',
     'mmlu_jurisprudence_test',
     'mmlu_logical_fallacies_test',
     'mmlu_us_foreign_policy_test',
     'mmlu_high_school_statistics_test',
     'mmlu_virology_test',
     'mmlu_formal_logic_test',
     'mmlu_security_studies_test',
     'mmlu_machine_learning_test',
     'mmlu_high_school_us_history_test',
     'mmlu_world_religions_test',
     'mmlu_high_school_chemistry_test',
     'mmlu_prehistory_test',
     'mmlu_electrical_engineering_test',
     'mmlu_high_school_european_history_test',
     'mmlu_high_school_psychology_test',
     'mmlu_high_school_world_history_test',
     'mmlu_high_school_geography_test',
     'mmlu_high_school_computer_science_test',
     'mmlu_human_aging_test',
     'mmlu_marketing_test',
     'mmlu_high_school_mathematics_test',
     'mmlu_conceptual_physics_test',
     'mmlu_abstract_algebra_test',
     'mmlu_professional_psychology_test',
     'mmlu_management_test',
     'mmlu_high_school_macroeconomics_test',
     'mmlu_sociology_test',
     'mmlu_nutrition_test',
     'mmlu_college_biology_test',
     'mmlu_professional_law_test',
     'mmlu_astronomy_test',
     'mmlu_college_physics_test',
     'mmlu_miscellaneous_test',
     'mmlu_high_school_microeconomics_test',
     'mmlu_computer_security_test',
     'mmlu_international_law_test',
     'mmlu_global_facts_test',
     'mmlu_human_sexuality_test',
     'mmlu_econometrics_test',
     'mmlu_anatomy_test',
     'mmlu_medical_genetics_test',
     'mmlu_college_medicine_test',
     'mmlu_high_school_physics_test',
     'mmlu_college_chemistry_test']


###############################################
# utils imported into eval_metrics.py
###############################################

def get_gt_file_path(file):
    """ If return updated path to a dev|test|train.tsv file using
        UQA_DIR path on current machine
        usage:
            get_gt_file_path('/old/uqa/dataset/dev.tsv') -> 'current/uqa/dataset/dev.tsv' 
            get_gt_file_path('dev.tsv') -> /current/uqa/dev.tsv without dataset name
            get_gt_file_path('/old/uqa/dataset') -> 'current/uqa/dataset'
    """
    curr_dir, probably_file = os.path.split(file)
    if probably_file.strip().endswith('.tsv'):  # assume /old/uqadir/dataset/dev|train|test.tsv
        curr_uqa_dir, dataset = os.path.split(curr_dir)
        return os.path.join(UQA_DIR, dataset, probably_file)
    else:                                       # assume /old/uqadir/dataset
        return os.path.join(UQA_DIR, probably_file)        


########################################################
# Eval Datasets q[+mc]->a to take as input and output to new dir as q[+mc]+e->a i.e to generate e for
# New dynamically created datasets will be created as e.g /UQA_DIR/qasc_dyn_expl_ans_modeloutputdir_timestamp
# NOTE: Each of these datasets must be in dataset_attribs and in one of dev_eval or test_eval as using the dev.tsv or test.tsv will be inferred from this
# NOTE2: The dynamically created versions will be added "on the fly" to dev_eval and test_eval and to a special "unseen" dataset
########################################################

#create_datasets_dynamic = ['musique_mu_dev_qa', 'strategy_qa_od_ans', 'qasc', 'arc_easy', 'arc_hard', 'nq_open_od_ans', 'arc_da_od_ans' ]
create_datasets_dynamic = []

# Not used
unifiedqa_unseen_5 = []

def list_files_pattern(dirtolist, pattern='*'):
    """ Returns a list of files in a dictionary matching a pattern
    """
    return [file for file in os.listdir(dirtolist) if fnmatch.fnmatch(file, pattern)]


for dset in create_datasets_dynamic:
    curr_dir = os.path.join(UQA_DIR, dset)
    if not os.path.exists(curr_dir):
        print(f"ERROR: dataset_attributes.py: {curr_dir} doesn't exist! Skipping...")
        continue
    attrib = dataset_attribs.get(dset)
    if attrib is None:
        print(f"ERROR: dataset_attributes.py: {attrib} hasn't been added to dataset_attribs! Skipping...")
        continue
    if dset in dev_eval:
        evaltype = 'dev'
    elif dset in test_eval:
        evaltype = 'test'
    else:
        print(f"ERROR: dataset_attributes.py: {dset} hasn't been added to one of dev_eval or test_eval! Skipping...")
        continue
    dyn_dsets = list_files_pattern(UQA_DIR, dset+SVISED_EXPL_ANS+'*')  
    if dyn_dsets == []:
        print(f"WARNING: dataset_attributes.py: No dynamically created datasets from {dset} were found in {UQA_DIR}! Skipping...")
        continue
    for dyn_ds in dyn_dsets:
        dataset_attribs[dyn_ds] = attrib
        if evaltype == 'dev':
            if dyn_ds not in dev_eval:
                dev_eval.append(dyn_ds)
        else:
            if dyn_ds not in test_eval:
                test_eval.append(dyn_ds)
        unifiedqa_unseen_5.append(dyn_ds)    
unifiedqa_unseen_5.sort()        

